{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "138370ac-4283-4639-bd27-8822e912f5d6",
   "metadata": {},
   "source": [
    "# Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7fdcfa-1b1b-4cbd-98dd-b72488a82ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note : Can only do one word per time as our lexicon cover unstemmed words also but not other dictionary except PRPM do that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75206d9f-df3c-447f-9194-70234d0f9750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/de-prj/de-venv/lib/python3.10/site-packages/malaya/tokenizer.py:214: FutureWarning: Possible nested set at position 3397\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n",
      "/home/student/de-prj/de-venv/lib/python3.10/site-packages/malaya/tokenizer.py:214: FutureWarning: Possible nested set at position 3927\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n"
     ]
    }
   ],
   "source": [
    "from PyClasses.Scrapers.PRPMScraper import PRPMScraper\n",
    "from PyClasses.Scrapers.WikiScrap import WikiScraper\n",
    "from PyClasses.Preprocessing.PRPMCleaner import PRPMCleaner\n",
    "from PyClasses.Neo4j.lexicon_nodes import LexiconNodeManager\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6796487d-e73f-4304-aa5e-4f907ce9c0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Established Connection\n",
      "Successfully run \n",
      "    MATCH (n:WORD {word: 'buku'})\n",
      "    RETURN n.word AS word, n.definitions AS definitions\n",
      "    LIMIT 1\n",
      "    !\n",
      "[{'word': 'buku', 'definitions': ['berhelai-helai kertas berjilid yang berisi tulisan untuk dibaca atau berisi ruangan-ruangan kosong untuk ditulisi']}]\n"
     ]
    }
   ],
   "source": [
    "# Neo4j\n",
    "uri = \"neo4j+s://bb539a73.databases.neo4j.io\"\n",
    "auth= (\"neo4j\", \"nHf-IeFSY8xVQvFU4UEaLs0ugES0uD7R-lR8mGgxt9w\")\n",
    "\n",
    "lnm = LexiconNodeManager(uri, auth)\n",
    "\n",
    "word_to_find = \"buku\" \n",
    "single_word_query = f\"\"\"\n",
    "    MATCH (n:WORD {{word: '{word_to_find}'}})\n",
    "    RETURN n.word AS word, n.definitions AS definitions\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "\n",
    "neoresult = lnm.create_custom_query(single_word_query)\n",
    "print(neoresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "510844a2-419f-46c2-a522-08c5bb6e4df6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cari': {'WikiDefi': ['Berusaha menjumpai sesuatu yang sukar/tidak kelihatan atau hilang.']}}\n"
     ]
    }
   ],
   "source": [
    "wk = WikiScraper()\n",
    "word = 'cari'\n",
    "wikiresult = wk.findWordMetaData(word)\n",
    "\n",
    "if wikiresult and 'meanings' in wikiresult[word]:\n",
    "    raw_meanings = wikiresult[word]['meanings']\n",
    "    cleaned_meanings = prpm_cleaner.clean_all_meanings(raw_meanings)\n",
    "    wikiresult[word]['meanings'] = cleaned_meanings\n",
    "\n",
    "print(wikiresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "171ce6a8-3454-4674-926b-51ae59a1ca01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berusaha menjumpai sesuatu yang sukar/tidak kelihatan atau hilang.\n"
     ]
    }
   ],
   "source": [
    "wiki_meaning = wikiresult[word]['WikiDefi'][0]\n",
    "print(wiki_meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "328443d1-ccf1-4969-9dca-d98f700f3e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "berhelai-helai kertas berjilid yang berisi tulisan untuk dibaca atau berisi ruangan-ruangan kosong untuk ditulisi\n"
     ]
    }
   ],
   "source": [
    "neo_combined = ' '.join(neoresult[0]['definitions'])\n",
    "print(neo_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71406999-687d-41ed-b775-e056cb84009e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Overlap Accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove all punctuation\n",
    "    text = ' '.join(text.split())       # Normalize multiple spaces to a single space\n",
    "    return text.lower()                 # Convert to lowercase\n",
    "\n",
    "# Clean the input texts\n",
    "wiki_cleaned = clean_text(wiki_meaning)\n",
    "neo_cleaned = clean_text(neo_combined)\n",
    "\n",
    "# Tokenize after cleaning\n",
    "wiki_tokens = set(wiki_cleaned.split())\n",
    "neo_tokens = set(neo_cleaned.split())\n",
    "\n",
    "# Calculate intersection and accuracy\n",
    "intersection = wiki_tokens & neo_tokens\n",
    "accuracy = len(intersection) / len(wiki_tokens)\n",
    "\n",
    "# Output\n",
    "print(f\"Token Overlap Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c95dbe-2371-4de6-8326-a8f08777d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wiki_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b113f-23f2-43a3-99cb-1dc3ffe9dabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neo_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcb2640-841f-43d8-b99b-b51c0b9c4223",
   "metadata": {},
   "source": [
    "# Coverage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e2285-1374-488e-933b-b4cc62584524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyClasses.Neo4j.lexicon_nodes import LexiconNodeManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dfd3b8-045f-4aca-80f3-86ff6d2dd551",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"neo4j+s://bb539a73.databases.neo4j.io\"\n",
    "auth= (\"neo4j\", \"nHf-IeFSY8xVQvFU4UEaLs0ugES0uD7R-lR8mGgxt9w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922ad91-f3da-45ac-b4f7-aeb465e6cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnm = LexiconNodeManager(uri, auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827960d-3554-4696-bffa-e5d6f212b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_query = \"\"\"\n",
    "    MATCH (n:WORD)\n",
    "    RETURN n.word AS word, n.word_count AS count, n.definitions AS definitions\n",
    "    ORDER BY n.word_count DESC\n",
    "    LIMIT 1000\n",
    "    \"\"\"\n",
    "\n",
    "result = lnm.create_custom_query(analysis_query)\n",
    "\n",
    "# Analyze coverage\n",
    "total_words = len(result)\n",
    "covered_words = 0\n",
    "not_covered_words = []\n",
    "\n",
    "for word in result:\n",
    "    definitions = word['definitions']\n",
    "    if definitions and any(defn.strip() for defn in definitions):\n",
    "        covered_words += 1\n",
    "    else: \n",
    "        not_covered_words.append(word['word'])\n",
    "\n",
    "coverage_percentage = (covered_words / total_words) * 100 if total_words > 0 else 0\n",
    "\n",
    "print(f\"Total Words: {total_words}\")\n",
    "print(f\"Covered Words: {covered_words}\")\n",
    "print(f\"Coverage Percentage: {coverage_percentage:.2f}%\")\n",
    "\n",
    "print(\"\\nExample of Not Covered Words:\")\n",
    "print(\", \".join(not_covered_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b3345-afa3-48d1-b00c-8677bd4ab267",
   "metadata": {},
   "source": [
    "# Consistency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d4d58e-372e-4c33-ba6b-c7ec7a141d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency : \n",
    "# If the model consistently labels words as verbs (even when they are not), \n",
    "# it shows that the model is following a consistent pattern \n",
    "# (predicting verbs) across different inputs.\n",
    "# Shit accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85b22f05-0040-4522-9042-56440287f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = [\"perempuan\", \"polis\", \"guru\", \"rumah\", \"buku\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab76149e-f50b-48dc-bdc7-ff6be9c309ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Established Connection\n",
      "Successfully run \n",
      "        MATCH (n:WORD {word: 'perempuan'})\n",
      "        RETURN n.word AS word, n.POS AS pos\n",
      "        LIMIT 1\n",
      "        !\n",
      "[{'word': 'perempuan', 'pos': 'Proper noun'}]\n",
      "Successfully run \n",
      "        MATCH (n:WORD {word: 'polis'})\n",
      "        RETURN n.word AS word, n.POS AS pos\n",
      "        LIMIT 1\n",
      "        !\n",
      "[{'word': 'polis', 'pos': 'Proper noun'}]\n",
      "Successfully run \n",
      "        MATCH (n:WORD {word: 'guru'})\n",
      "        RETURN n.word AS word, n.POS AS pos\n",
      "        LIMIT 1\n",
      "        !\n",
      "[{'word': 'guru', 'pos': 'Proper noun'}]\n",
      "Successfully run \n",
      "        MATCH (n:WORD {word: 'rumah'})\n",
      "        RETURN n.word AS word, n.POS AS pos\n",
      "        LIMIT 1\n",
      "        !\n",
      "[{'word': 'rumah', 'pos': 'Proper noun'}]\n",
      "Successfully run \n",
      "        MATCH (n:WORD {word: 'buku'})\n",
      "        RETURN n.word AS word, n.POS AS pos\n",
      "        LIMIT 1\n",
      "        !\n",
      "[{'word': 'buku', 'pos': 'Proper noun'}]\n"
     ]
    }
   ],
   "source": [
    "from PyClasses.Neo4j.lexicon_nodes import LexiconNodeManager\n",
    "\n",
    "uri = \"neo4j+s://bb539a73.databases.neo4j.io\"\n",
    "auth= (\"neo4j\", \"nHf-IeFSY8xVQvFU4UEaLs0ugES0uD7R-lR8mGgxt9w\")\n",
    "\n",
    "lnm = LexiconNodeManager(uri, auth)\n",
    "\n",
    "for word in lists: \n",
    "    single_word_query = f\"\"\"\n",
    "        MATCH (n:WORD {{word: '{word}'}})\n",
    "        RETURN n.word AS word, n.POS AS pos\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "    neoresult = lnm.create_custom_query(single_word_query)\n",
    "    print(neoresult) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361626cc-8bc8-42f6-a6be-512f1e6915d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "de-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
