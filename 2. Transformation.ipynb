{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca5d504-162a-4edd-9f29-2e36c4bd3b45",
   "metadata": {},
   "source": [
    "## Data Transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d3ec20-3f3a-4dad-9199-254bef0b9634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Data Transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/22 15:50:22 WARN Utils: Your hostname, Liang. resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/12/22 15:50:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/12/22 15:50:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data From Kafka...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropped duplicated articles which exist in database(DE-prj/RawData),\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles left to be processed: 10\n",
      "Breaking raw data into list of sentences...\n",
      "Breaking sentences into words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://localhost:9000/user/student/DE-prj/MR_WC_Result\n",
      "['/home/hduser/hadoop3/bin/hadoop', 'jar', '/home/hduser/hadoop3/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar', '-input', 'hdfs://localhost:9000/user/student/DE-prj/TempWords/part-00000-788ea45f-36f6-4de8-8e00-dbdbb40419f1-c000.txt,hdfs://localhost:9000/user/student/DE-prj/TempWords/part-00001-788ea45f-36f6-4de8-8e00-dbdbb40419f1-c000.txt,hdfs://localhost:9000/user/student/DE-prj/TempWords/part-00002-788ea45f-36f6-4de8-8e00-dbdbb40419f1-c000.txt,hdfs://localhost:9000/user/student/DE-prj/TempWords/part-00003-788ea45f-36f6-4de8-8e00-dbdbb40419f1-c000.txt,hdfs://localhost:9000/user/student/DE-prj/TempWords/part-00004-788ea45f-36f6-4de8-8e00-dbdbb40419f1-c000.txt,hdfs://localhost:9000/user/student/DE-prj/TempWords/part-00005-788ea45f-36f6-4de8-8e00-dbdbb40419f1-c000.txt,hdfs://localhost:9000/user/student/DE-prj/TempWords/part-00006-788ea45f-36f6-4de8-8e00-dbdbb40419f1-c000.txt,hdfs://localhost:9000/user/student/DE-prj/TempWords/part-00007-788ea45f-36f6-4de8-8e00-dbdbb40419f1-c000.txt,hdfs://localhost:9000/user/student/DE-prj/TempWords/part-00008-788ea45f-36f6-4de8-8e00-dbdbb40419f1-c000.txt,hdfs://localhost:9000/user/student/DE-prj/TempWords/part-00009-788ea45f-36f6-4de8-8e00-dbdbb40419f1-c000.txt,hdfs://localhost:9000/user/student/DE-prj/TempWords/part-00010-788ea45f-36f6-4de8-8e00-dbdbb40419f1-c000.txt,hdfs://localhost:9000/user/student/DE-prj/TempWords/part-00011-788ea45f-36f6-4de8-8e00-dbdbb40419f1-c000.txt', '-output', 'hdfs://localhost:9000/user/student/DE-prj/MR_WC_Result', '-mapper', 'lexicraft/MapReduce/WordCount/mapper.py', '-reducer', 'lexicraft/MapReduce/WordCount/reducer.py', '-file', 'lexicraft/MapReduce/WordCount/mapper.py', '-file', 'lexicraft/MapReduce/WordCount/reducer.py']\n",
      "Running Hadoop MapReduce for word count...\n",
      "Hadoop Streaming Command Output:\n",
      "packageJobJar: [lexicraft/MapReduce/WordCount/mapper.py, lexicraft/MapReduce/WordCount/reducer.py, /tmp/hadoop-unjar9031693158223908835/] [] /tmp/streamjob221719218610262497.jar tmpDir=null\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raw data to be appeneded:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data has been successfully appended to HDFS:  DE-prj/RawData\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence data has been successfully appended to HDFS:  DE-prj/Sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word data has been successfully appended to HDFS:  DE-prj/Words\n"
     ]
    }
   ],
   "source": [
    "from lexicraft import Transformation\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "def start_demo_transformation():\n",
    "    print(\"Performing Data Transformation...\")\n",
    "    spark = SparkSession.builder.appName(\"DE-prj\").getOrCreate()\n",
    "    transformation = Transformation(spark,\"beritaH\",\"localhost:9092\",0)\n",
    "    status = transformation.start_transform_rawdata_from_kafka()\n",
    "    if status == \"no article to be processed\":\n",
    "        print(\"No Article Links can scrap.\")\n",
    "        print(\"Exiting...\")\n",
    "        try:\n",
    "            spark.stop()\n",
    "        except:\n",
    "            return\n",
    "        \n",
    "    # spark.stop()\n",
    "    result = Transformation.wordCountMapReduce()\n",
    "    if result == \"fail\":\n",
    "        print(\"Fail to run Hadoop Map Reduce\")\n",
    "        spark.stop()\n",
    "        return\n",
    "    #else\n",
    "    status = transformation.save_temp_data_to_permanent()\n",
    "    if status == \"fail\":\n",
    "        print(\"Fail to save temp_data_to_permanent\")\n",
    "        spark.stop()\n",
    "        return\n",
    "\n",
    "start_demo_transformation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "de-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
