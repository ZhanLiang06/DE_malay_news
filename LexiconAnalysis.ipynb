{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f3c7e3-cf12-419f-832f-8c85420cee80",
   "metadata": {},
   "source": [
    "# Lexicon Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e8b2048-3a86-47eb-b8b5-3090f294551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"neo4j+s://bb539a73.databases.neo4j.io\"\n",
    "auth = (\"neo4j\", \"nHf-IeFSY8xVQvFU4UEaLs0ugES0uD7R-lR8mGgxt9w\") ## using liangzlau personal account\n",
    "spark = SparkSession.builder.appName(\"DE-prj\").getOrCreate()\n",
    "analysis_instance = Analysis(spark,uri,auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8650d9f3-b0c9-4219-a182-a4e708042dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Established Connection\n",
      "Successfully retrieved all words no. of words = 5276!\n",
      "Message sent to word_length_analysis partition 0 at offset 23\n"
     ]
    }
   ],
   "source": [
    "analysis_instance.word_length_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6962dda5-4243-4ac6-bde7-889f9fc6c586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import malaya\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from malaya.stem import sastrawi\n",
    "from pyspark.sql import Row\n",
    "from PyClasses.Neo4j.lexicon_nodes import LexiconNodeManager\n",
    "import malaya\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "# Function to analyze morphological structure using Sastrawi stemmer\n",
    "class Analysis():\n",
    "    def __init__(self,spark,uri,auth):\n",
    "        self.kafka_bootstrap_servers = \"localhost:9092\"\n",
    "        self.producer = KafkaProducer(bootstrap_servers=self.kafka_bootstrap_servers, value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "        self.spark = spark\n",
    "        self.uri = uri\n",
    "        self.auth = auth\n",
    "\n",
    "    def on_send_success(self, record_metadata):\n",
    "        print(f\"Message sent to {record_metadata.topic} partition {record_metadata.partition} at offset {record_metadata.offset}\")\n",
    "\n",
    "    def on_send_error(self, exception):\n",
    "        print(f\"Error occurred: {exception}\")\n",
    "    \n",
    "    def lemma_length_analysis(self):\n",
    "        lnm = LexiconNodeManager(self.uri, self.auth)\n",
    "        analysis_query = \"\"\"\n",
    "            MATCH (n:WORD)\n",
    "            RETURN n.base_word AS lemmatized\n",
    "        \"\"\"\n",
    "        all_words = lnm.create_custom_query(analysis_query)\n",
    "        words_list = [item['lemmatized'] for item in all_words]\n",
    "        word_lengths = [len(word) for word in words_list]\n",
    "        average_word_length = np.mean(word_lengths)\n",
    "        most_freq_length =  np.argmax(np.bincount(word_lengths))\n",
    "\n",
    "        message = {\"word_lengths\":word_lengths,\"avg_word_length\":float(average_word_length),\"most_freq_length\":int(most_freq_length)}\n",
    "        self.producer.send(\"lemma_length_analysis\", partition=0,value=message).add_callback(self.on_send_success).add_errback(self.on_send_error)\n",
    "        self.producer.flush()\n",
    "    \n",
    "    \n",
    "    def lexicon_analysis(self):\n",
    "        lnm = LexiconNodeManager(self.uri, self.auth)\n",
    "        # Return everything\n",
    "        query = \"\"\"\n",
    "        MATCH (n:WORD)\n",
    "        WITH COUNT(n) AS totalWords\n",
    "        MATCH (n:PERIBAHASA)\n",
    "        RETURN totalWords, COUNT(n) AS totalPeri;\n",
    "        \"\"\"\n",
    "        result = lnm.create_custom_query(query)\n",
    "        \n",
    "        message = result\n",
    "        self.producer.send(\"lexicon_analysis\", partition=0,value=message).add_callback(self.on_send_success).add_errback(self.on_send_error)\n",
    "        self.producer.flush()\n",
    "        \n",
    "\n",
    "    def word_length_analysis(self):\n",
    "        lnm = LexiconNodeManager(self.uri, self.auth)\n",
    "        all_words = lnm.get_all_words()\n",
    "        words_list = [item['n.word'] for item in all_words]\n",
    "        word_lengths = [len(word) for word in words_list]\n",
    "        average_word_length = np.mean(word_lengths)\n",
    "        most_freq_length =  np.argmax(np.bincount(word_lengths))\n",
    "\n",
    "        message = {\"word_lengths\":word_lengths,\"avg_word_length\":float(average_word_length),\"most_freq_length\":int(most_freq_length)}\n",
    "        self.producer.send(\"word_length_analysis\", partition=0,value=message).add_callback(self.on_send_success).add_errback(self.on_send_error)\n",
    "        self.producer.flush()\n",
    "\n",
    "    def sentiment_dist(self):\n",
    "        lnm = LexiconNodeManager(self.uri, self.auth)\n",
    "        # # Get all result\n",
    "        # result = lnm.get_all_words()\n",
    "        # result = [d[\"n.word\"] for d in result]\n",
    "        \n",
    "        # Word Frequency Analysis\n",
    "        analysis_query = \"\"\"\n",
    "            MATCH (n:WORD)\n",
    "            RETURN n.Label AS label, COUNT(n) AS total_count\n",
    "            ORDER BY total_count DESC\n",
    "            \"\"\"\n",
    "        result = lnm.create_custom_query(analysis_query)\n",
    "        \n",
    "        message = result\n",
    "        self.producer.send(\"sentiment_dist_analysis\", partition=0,value=message).add_callback(self.on_send_success).add_errback(self.on_send_error)\n",
    "        self.producer.flush()\n",
    "\n",
    "\n",
    "    def word_freq_with_stopwords(self):\n",
    "        lnm = LexiconNodeManager(self.uri, self.auth)\n",
    "        # # Get all result\n",
    "        # result = lnm.get_all_words()\n",
    "        # result = [d[\"n.word\"] for d in result]\n",
    "        # Word Frequency Analysis\n",
    "        analysis_query = \"\"\"\n",
    "            MATCH (n:WORD)\n",
    "            RETURN n.word AS word, n.word_count AS count\n",
    "            ORDER BY n.word_count DESC\n",
    "            LIMIT 15\n",
    "            \"\"\"\n",
    "        result = lnm.create_custom_query(analysis_query)    \n",
    "        \n",
    "        message = result\n",
    "        self.producer.send(\"word_freq_with_stopwords\", partition=0,value=message).add_callback(self.on_send_success).add_errback(self.on_send_error)\n",
    "        self.producer.flush()\n",
    "    \n",
    "    def word_freq_without_stopwords(self):\n",
    "        # Stopwords List, https://github.com/stopwords-iso/stopwords-ms\n",
    "        stopwords = [\n",
    "            'abdul', 'abdullah', 'acara', 'ada', 'adalah', 'ahmad', 'air', 'akan', 'akhbar', \n",
    "            'akhir', 'aktiviti', 'alam', 'amat', 'amerika', 'anak', 'anggota', 'antara', \n",
    "            'antarabangsa', 'apa', 'apabila', 'april', 'as', 'asas', 'asean', 'asia', 'asing', \n",
    "            'atas', 'atau', 'australia', 'awal', 'awam', 'bagaimanapun', 'bagi', 'bahagian', \n",
    "            'bahan', 'baharu', 'bahawa', 'baik', 'bandar', 'bank', 'banyak', 'barangan', \n",
    "            'baru', 'baru-baru', 'bawah', 'beberapa', 'bekas', 'beliau', 'belum', 'berada', \n",
    "            'berakhir', 'berbanding', 'berdasarkan', 'berharap', 'berikutan', 'berjaya', \n",
    "            'berjumlah', 'berkaitan', 'berkata', 'berkenaan', 'berlaku', 'bermula', 'bernama', \n",
    "            'bernilai', 'bersama', 'berubah', 'besar', 'bhd', 'bidang', 'bilion', 'bn', 'boleh', \n",
    "            'bukan', 'bulan', 'bursa', 'cadangan', 'china', 'dagangan', 'dalam', 'dan', 'dana', \n",
    "            'dapat', 'dari', 'daripada', 'dasar', 'datang', 'datuk', 'demikian', 'dengan', 'depan', \n",
    "            'derivatives', 'dewan', 'di', 'diadakan', 'dibuka', 'dicatatkan', 'dijangka', \n",
    "            'diniagakan', 'dis', 'disember', 'ditutup', 'dolar', 'dr', 'dua', 'dunia', 'ekonomi', \n",
    "            'eksekutif', 'eksport', 'empat', 'enam', 'faedah', 'feb', 'global', 'hadapan', 'hanya', \n",
    "            'harga', 'hari', 'hasil', 'hingga', 'hubungan', 'ia', 'iaitu', 'ialah', 'indeks', 'india', \n",
    "            'indonesia', 'industri', 'ini', 'islam', 'isnin', 'isu', 'itu', 'jabatan', 'jalan', 'jan', \n",
    "            'jawatan', 'jawatankuasa', 'jepun', 'jika', 'jualan', 'juga', 'julai', 'jumaat', 'jumlah', \n",
    "            'jun', 'juta', 'kadar', 'kalangan', 'kali', 'kami', 'kata', 'katanya', 'kaunter', 'kawasan', \n",
    "            'ke', 'keadaan', 'kecil', 'kedua', 'kedua-dua', 'kedudukan', 'kekal', 'kementerian', 'kemudahan', \n",
    "            'kenaikan', 'kenyataan', 'kepada', 'kepentingan', 'keputusan', 'kerajaan', 'kerana', 'kereta', \n",
    "            'kerja', 'kerjasama', 'kes', 'keselamatan', 'keseluruhan', 'kesihatan', 'ketika', 'ketua', \n",
    "            'keuntungan', 'kewangan', 'khamis', 'kini', 'kira-kira', 'kita', 'klci', 'klibor', 'komposit', \n",
    "            'kontrak', 'kos', 'kuala', 'kuasa', 'kukuh', 'kumpulan', 'lagi', 'lain', 'langkah', 'laporan', \n",
    "            'lebih', 'lepas', 'lima', 'lot', 'luar', 'lumpur', 'mac', 'mahkamah', 'mahu', 'majlis', 'makanan', \n",
    "            'maklumat', 'malam', 'malaysia', 'mana', 'manakala', 'masa', 'masalah', 'masih', 'masing-masing', \n",
    "            'masyarakat', 'mata', 'media', 'mei', 'melalui', 'melihat', 'memandangkan', 'memastikan', 'membantu', \n",
    "            'membawa', 'memberi', 'memberikan', 'membolehkan', 'membuat', 'mempunyai', 'menambah', 'menarik', \n",
    "            'menawarkan', 'mencapai', 'mencatatkan', 'mendapat', 'mendapatkan', 'menerima', 'menerusi', \n",
    "            'mengadakan', 'mengambil', 'mengenai', 'menggalakkan', 'menggunakan', 'mengikut', 'mengumumkan', \n",
    "            'mengurangkan', 'meningkat', 'meningkatkan', 'menjadi', 'menjelang', 'menokok', 'menteri', \n",
    "            'menunjukkan', 'menurut', 'menyaksikan', 'menyediakan', 'mereka', 'merosot', 'merupakan', \n",
    "            'mesyuarat', 'minat', 'minggu', 'minyak', 'modal', 'mohd', 'mudah', 'mungkin', 'naik', 'najib', \n",
    "            'nasional', 'negara', 'negara-negara', 'negeri', 'niaga', 'nilai', 'nov', 'ogos', 'okt', 'oleh', \n",
    "            'operasi', 'orang', 'pada', 'pagi', 'paling', 'pameran', 'papan', 'para', 'paras', 'parlimen', \n",
    "            'parti', 'pasaran', 'pasukan', 'pegawai', 'pejabat', 'pekerja', 'pelabur', 'pelaburan', 'pelancongan', \n",
    "            'pelanggan', 'pelbagai', 'peluang', 'pembangunan', 'pemberita', 'pembinaan', 'pemimpin', \n",
    "            'pendapatan', 'pendidikan', 'penduduk', 'penerbangan', 'pengarah', 'pengeluaran', 'pengerusi', \n",
    "            'pengguna', 'pengurusan', 'peniaga', 'peningkatan', 'penting', 'peratus', 'perdagangan', 'perdana', \n",
    "            'peringkat', 'perjanjian', 'perkara', 'perkhidmatan', 'perladangan', 'perlu', 'permintaan', \n",
    "            'perniagaan', 'persekutuan', 'persidangan', 'pertama', 'pertubuhan', 'pertumbuhan', 'perusahaan', \n",
    "            'peserta', 'petang', 'pihak', 'pilihan', 'pinjaman', 'polis', 'politik', 'presiden', 'prestasi', \n",
    "            'produk', 'program', 'projek', 'proses', 'proton', 'pukul', 'pula', 'pusat', 'rabu', 'rakan', 'rakyat', \n",
    "            'ramai', 'rantau', 'raya', 'rendah', 'ringgit', 'rumah', 'sabah', 'sahaja', 'saham', 'sama', 'sarawak', \n",
    "            'satu', 'sawit', 'saya', 'sdn', 'sebagai', 'sebahagian', 'sebanyak', 'sebarang', 'sebelum', 'sebelumnya', \n",
    "            'sebuah', 'secara', 'sedang', 'segi', 'sehingga', 'sejak', 'sekarang', 'sektor', 'sekuriti', 'selain', \n",
    "            'selama', 'selasa', 'selatan', 'selepas', 'seluruh', 'semakin', 'semalam', 'semasa', 'sementara', \n",
    "            'semua', 'semula', 'sen', 'sendiri', 'seorang', 'sepanjang', 'seperti', 'sept', 'september', \n",
    "            'serantau', 'seri', 'serta', 'sesi', 'setiap', 'setiausaha', 'sidang', 'singapura', 'sini', 'sistem', \n",
    "            'sokongan', 'sri', 'sudah', 'sukan', 'suku', 'sumber', 'supaya', 'susut', 'syarikat', 'syed', 'tahap', \n",
    "            'tahun', 'tan', 'tanah', 'tanpa', 'tawaran', 'teknologi', 'telah', 'tempat', 'tempatan', 'tempoh', \n",
    "            'tenaga', 'tengah', 'tentang', 'terbaik', 'terbang', 'terbesar', 'terbuka', 'terdapat', 'terhadap', \n",
    "            'termasuk', 'tersebut', 'terus', 'tetapi', 'thailand', 'tiada', 'tidak', 'tiga', 'timbalan', 'timur', \n",
    "            'tindakan', 'tinggi', 'tun', 'tunai', 'turun', 'turut', 'umno', 'unit', 'untuk', 'untung', 'urus', 'usaha', \n",
    "            'utama', 'walaupun', 'wang', 'wanita', 'wilayah', 'yang'\n",
    "        ]\n",
    "        lnm = LexiconNodeManager(self.uri, self.auth)\n",
    "\n",
    "        \n",
    "        # Word Frequency Analysis\n",
    "        analysis_query = \"\"\"\n",
    "            MATCH (n:WORD)\n",
    "            RETURN n.word AS word, n.word_count AS count\n",
    "            ORDER BY n.word_count DESC\n",
    "            LIMIT 1500\n",
    "            \"\"\"\n",
    "    \n",
    "        result = lnm.create_custom_query(analysis_query)\n",
    "        result = [item for item in result if item['word'] not in stopwords]\n",
    "        top_15_result = result[:15]\n",
    "\n",
    "        message = top_15_result\n",
    "        self.producer.send(\"word_freq_without_stopwords\", partition=0,value=message).add_callback(self.on_send_success).add_errback(self.on_send_error)\n",
    "        self.producer.flush()\n",
    "\n",
    "    \n",
    "    def POSDistribution(self):\n",
    "        lnm = LexiconNodeManager(self.uri, self.auth)\n",
    "        # # Get all result\n",
    "        # result = lnm.get_all_words()\n",
    "        # result = [d[\"n.word\"] for d in result]\n",
    "        \n",
    "        # Word Frequency Analysis\n",
    "        analysis_query = \"\"\"\n",
    "            MATCH (n:WORD)\n",
    "            RETURN n.POS AS pos, COUNT(n) AS total_count\n",
    "            ORDER BY total_count DESC\n",
    "            \"\"\"\n",
    "        result = lnm.create_custom_query(analysis_query)    \n",
    "        \n",
    "        message = result\n",
    "        self.producer.send(\"POSDistribution\", partition=0,value=message).add_callback(self.on_send_success).add_errback(self.on_send_error)\n",
    "        self.producer.flush()\n",
    "\n",
    "        \n",
    "    def morphological_analysis(self):\n",
    "        def analyze_token(tokens):\n",
    "            # token_stem_tuple = []\n",
    "            # prefix_list = []\n",
    "            # suffix_list = []\n",
    "            result = []\n",
    "            stemmer = sastrawi()\n",
    "            for token in tokens:\n",
    "                stem = stemmer.stem(token)  # Get the root/stem of the word\n",
    "                prefix = None\n",
    "                suffix = None\n",
    "                core_word = stem\n",
    "            \n",
    "                # Check if the token has a prefix and/or suffix\n",
    "                if token != stem:\n",
    "                    # The stemmer removes affixes; calculate what was removed\n",
    "                    prefix = token[:token.index(stem)] if stem in token else None\n",
    "                    suffix = token[token.index(stem) + len(stem):] if stem in token else None\n",
    "                \n",
    "                result.append( {\n",
    "                    'word': token,\n",
    "                    'stem': stem,\n",
    "                    'prefix': prefix,\n",
    "                    'suffix': suffix,\n",
    "                    'core_word': stem.strip()  # Root word after removing affixes\n",
    "                })\n",
    "\n",
    "            return result\n",
    "\n",
    "        lnm = LexiconNodeManager(self.uri, self.auth)\n",
    "        # Get all result\n",
    "        word_list = lnm.get_all_words()\n",
    "        word_list = [d[\"n.word\"] for d in word_list]\n",
    "        word_list_rdd = self.spark.sparkContext.parallelize(word_list)\n",
    "        result = word_list_rdd.mapPartitions(analyze_token).collect()\n",
    "        \n",
    "    \n",
    "        df_result = pd.DataFrame(result)\n",
    "\n",
    "        message = df_result.to_json(orient='records')\n",
    "        self.producer.send(\"morphological_analysis\", partition=0,value=message).add_callback(self.on_send_success).add_errback(self.on_send_error)\n",
    "        self.producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b2966-91d3-47a8-b7f7-eb8e481a531f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cats\n",
      "1 cats\n",
      "2 cats\n",
      "3 cats\n",
      "4 cats\n",
      "5 cats\n",
      "6 cats\n",
      "7 cats\n",
      "8 cats\n",
      "9 cats\n",
      "10 cats\n",
      "11 cats\n",
      "12 cats\n",
      "13 cats\n",
      "14 cats\n",
      "15 cats\n",
      "16 cats\n",
      "17 cats\n",
      "18 cats\n",
      "19 cats\n",
      "20 cats\n",
      "21 cats\n",
      "22 cats\n",
      "23 cats\n",
      "24 cats\n",
      "25 cats\n",
      "26 cats\n",
      "27 cats\n",
      "28 cats\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "import time\n",
    "import socket\n",
    "\n",
    "bootstrap_servers = \"localhost:9092\"\n",
    "topic = 'cats'\n",
    "time_interval = 1\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "for num in range(1000000):\n",
    "    message = f\"{num} cats\".encode('utf-8')\n",
    "    print(message.decode('utf-8'))\n",
    "    producer.send(topic, message)\n",
    "    time.sleep(time_interval)\n",
    "\n",
    "producer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "de-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
